%--------------------------------------------------------------------------------
%--------------------------------------------------------------------------------
%    CHAPTER III: TYPES AND STRUCTURES
%--------------------------------------------------------------------------------
%--------------------------------------------------------------------------------

% This is the mathematics chapter.

\part*{Types and Structures}
\addcontentsline{toc}{part}{\tocpartglyph Types and Structures}

% “My math background made me realize that each object could have several algebras associated with it, and there could be families of these, and that these would be very very useful.” - Alan Kay

% HUGE POINT: An object consists of a type and a structure. A type is what the object is semantically. What does the object mean? A structure is what the object is syntactically. What is the object made of and how is it arranged?

% Structures, not objects. For example, we are directly interested in list structure and only indirectly interested in objects *with* list structure. We focus on Forms, not on complex particulars.

% An abstract structure of fixed information content takes up a fixed amount of computational space. What is the difference between a structure and a space? A structure is a space with an associated type that represents constraints. A structure is a particular kind of space. (Look at quotes on space wiki).

% Why leave set theory? We want to explore proper classes, so we generalize set theory to category theory. Sets become objects and functions become morphisms.
% Type theory is a formal syntactic language of calculus for category theory. Category theory provides semantics for type theory.
% A mathematical object in something that exists in a mathematical universe. They can have many different kinds of structure. We are interested specifically in the \textit{algebraic structure} of objects.
% An automaton is a mathematical object
% Are objects and relations fundamentally different?

%--------------------------------------------------------------------------------
%    SECTION
%--------------------------------------------------------------------------------

\toclineskip
\section{Type Theory and Category Theory}

% Intentionality and extensionality - types are intensional, classes can be either (wiki - Class (philosophy))
% In this case, class is equivalent to the idea of a set in mathematics. Sets can also be defined extensionally and intensionally.
% In mathematics, the word class has different definitions based on which foundation of mathematics you are working in. Informally, in mathematics, a class is a collection of mathematical objects. In Zermelo-Fraenkel set theory, a class is informally a collection of sets (which are formally defined, axiomatized). In Von Neumann-Bernays-Godel set theory, the word class refers to the abstract idea of a proper class, which is formally defined. In ZFC, sets are elements of other sets, but classes are not elements of any other class (e.g. the class of ordinal numbers is not a set). In ZFC, sets are objects in your universe, classes are not. Sets are semantic objects, classes are syntactic objects.
% Before the foundational crisis of mathematics, the word set was used as an informal term for a collection of things. Set theory formalizes the definition of a set. Cantor's and Russell's paradox revealed that not all collections are sets. A class is any object that can be described in the langauge of set theory. Not all classes are sets. The axioms of set theory apply only to the sets.
% A category in category theory is a pair of classes (basically). In this case, the word class means either set or proper class. If both of its classes are actually just sets, the category is \textit{small}. Otherwise, it is \textit{large}.

% Essence - philosophy
% Curry-Howard Isomorphism
% Not all data structures implement abstract data types.

% Imperative vs functional ADTs

% Talk about how relations are sets (objects)
% Functions are deterministic relations

% In compsci, Method (can be abstract) vs effective method (steps fully described and finite, can be written in pseudocode) vs algorithm (steps fully described and finite, can be written in pseudocode, calculates the values of a function) vs an implementation of that algorithm. Church-Turing theorem, computability theory and what not... Talk about how every iterative function can be written recursively and vice versa. Any time you use recursion, you're basically adding a stack to the algorithm.

% Contiguous or non-contiguous? Array or node?
% Mention other ADTs that a DS could implement
% Address lack of pictures. Add pictures for ADTs?
% Delete operation should acutally be remove. Discuss the distinction.

%----------------------------------------

\subsection{The Curry-Howard-Lambek Isomorphism}

% Recursive composite type = a dynamic data structure

% Curry-Howard Isomorphism

% Logic programming

% Monoids, groups, algebras, coalgebras, etc.
% Signatures

% F-Coalgebra
% Semiautomata and transition systems
% Algebraic specification

%--------------------------------------------------------------------------------
%    SECTION
%--------------------------------------------------------------------------------

\toclineskip
\section{Abstract Data Types and the Data Structures that Implement Them}

% ADTs are "types" of metaphysical objects. They are 

% These are just patterns that appear in life. Trees tend to be more common in nature, lists are how humans tend to organize things.
% Do "plain old data structures" first: arrays, structs, records, unions, objects
% Data type + data structure wiki

% The subsections should be ADTs. The subsubsections should be data structures. If you really want to discuss a specialized implementation, you can make it paragraph or a blue box. But you should seriously consider pushing the implementations into an appendix for better thematic cohesion.

% What do I mean when I say... type (or data type), structure (or data structure), object, instance, class (of objects), implementation
% If an element has type $X$, it belongs to a class $X$. We typically speak in terms of types in theoretical computer science (harkening back to Alonzo Church) and in terms of classes in object-oriented programming (harkening back to the programming language Simula, whose designers Ole-Johan Dahl and Kristen Nygaard were harkening back to the idea of a class in philosophy and mathematics).

In this section, we evaluate data structures from a theoretical standpoint, describing their essential properties and why one may be preferable to another for some task. \textit{There is no single, best way of storing data.} The optimal layout varies depending on the nature of the task. The rules and requirements that define these data structures can be implemented in code, and many programming languages have their own implementations of these structures. \\

Some operations are common among many different data structures. The time complexity of any of these operations will depend on the choice of data structure. We will discuss how these operations are performed concretely for each data structure listed below, but first we will give an abstract description of these operations. \\

\begin{description}
	\item[Access] To \textit{access} an item in a data structure is to \textbf{provide} an index to denotes the desired element's location relative to the other elements (1st, 2nd, ..., Nth) and to \textbf{use} that element to acquire the element's data. The action is only applicable to data structures that have order.
	\item[Search] To \textit{search} for an element in a data structure is to \textbf{provide} some sort of information about the element (like its value) and to \textbf{use} that information to acquire the element's data.
	\item[Insert] To \textit{insert} an element into a data structure is to \textbf{find} a valid location to insert the element, to \textbf{allocate} that memory for use by the data structure, and to \textbf{store} the element's data there. Requirements for proper insertion depend on the data structure in question. For example, if the structure requires that its elements be sorted, an element must be inserted into that structure in sorted order. It also may be possible to insert an element into a specified location in the structure, with different locations taking different amounts of time.
	
	The \textit{reassign} operation is similar to insert, but simpler. It consists of finding a valid location and storing data there, no new memory is allocated. Instead, memory that is already a part of the data structure is overwritten.
	\item[Delete] To delete an element from a data structure is to \textbf{find} where the element is stored in memory and to \textbf{deallocate} that section of memory from the total memory controlled by the data structure. The data structure may also have to be reorganized in some way after this deallocation. Like insert, this operation has a runtime that depends heavily on where the element in located within the structure.
\end{description}
\vspace{5mm}

Note also that, in many cases, a name can be used to refer to both an abstract data type and a data structure that is an implementation of that type. For this reason, a table of relevant abstract data types is given below.
	
\begin{table}[H]
	\caption{Abstract Data Types}
	\label{tab:adts}
	\begin{tabularx}{\textwidth}{|l|X|}
		\vtabularspace{2}
		\hline
		\multicolumn{1}{|c|}{Abstract Data Type} & \multicolumn{1}{c|}{Structure Description} \\
		\hline
		Set & An unordered collection of unique values. \\
		\hline
		Multiset (or Bag) & An unordered collection of elements that are not necessarily unique. \\
		\hline
		List & An ordered collection of elements that are not necessarily unique. \\
		\hline
		Map & A collection of key-value pairs such that the keys are unique. \\
		\hline
		Graph & A set of elements (called nodes) and a set of pairs of those nodes (called edges). The pairs are unordered for an undirected graph or ordered for a directed graph.\\
		\hline
		Tree & A directed, acyclic graph in which all nodes have an in-degree of 1 (except the root node, which has an in-degree of 0). \\
		\hline
		Stack & A LIFO-ordered collection of elements that are not necessarily unique.\\
		\hline
		Queue & A FIFO-ordered collection of elements that are not necessarily unique. \\
		\hline
		Priority Queue & A priority-ordered collection of elements that are not necessarily unique. \\
		\hline
		Double-ended Queue & A queue that can add/remove elements from both ends. \\
		\hline
		Double-ended Priority Queue & A priority queue that can add/remove elements from both ends. \\
		\hline
		\vtabularspace{2}
	\end{tabularx}
\end{table}

Each data structure described below could implement one or more of these abstract data types. However, each name used below refers to a data structure of one of these types, not to the type itself, unless specified otherwise. \\

%----------------------------------------

% Iterators

\subsection{Lists}

% Talk about tuples, how they are different than lists, and why the preferred term is list (streams, countable infinity)

A \textit{list} is an abstract data type that represents an ordered collection of elements that are not necessarily unique. It is typically implemented with either static or dynamic arrays or with linked lists. The major difference between these data structures has to do with how the data is stored in memory (i.e. whether data is stored contiguously or not and whether the amount of allocated memory is fixed or not).

\subsubsection{Arrays}

*** Talk about how strings are often implemented with arrays. \\

An \textit{array} is a collection of elements that are stored contiguously in memory. Each element in the array is assigned an index which describes its relative position in the array.\\

The elements should take up the same amount of space in memory. This is required to allow indexing to function correctly. To find an element, you must find the beginning of that element's data in memory and then fetch a number of bits that corresponds to the element's size. To find the element with index $i$, you would start at the memory address where the array begins, skip forward $i\times$\texttt{elementSize} bits, and then fetch \texttt{elementSize} bits. In addition to elements taking up the same amount of space, they should also have the same type. This allows the bits to be interpreted in an unambiguous way.\\

Arrays can be \textit{static} or \textit{dynamic}. A static array takes up a fixed amount of size in memory while a dynamic array can be resized. The difficulty with increasing the size of an array is that an array must remain contiguous, but the memory just ahead of the end of the array could contain important data that cannot be erased. For this reason, dynamic arrays are often implemented as static arrays that are destroyed and recreated with a larger size elsewhere in memory when they exceed their previous size.

\begin{table}[H]
	\caption{Dynamic Array Time Complexity}
	\label{tab:array}
	\begin{tabularx}{\textwidth}{|c|c|X|}
		\vtabularspace{3}
		\hline
		Operation & Time & \multicolumn{1}{c|}{Reasoning} \\
		\hline
		Access & $O(1)$ & Indicies allow for direct access \\
		Search & $O(n)$ & Have to check elements linearly \\
		\hline
		\hline
		Insert (beginning) & $O(n)$ & Have to shift all elements one to the right \\
		Insert (middle) & $O(n)$ & Same as above in the worst case \\
		Insert (end) & $O(1)$ amortized & If there is allocated space at the end, it takes constant time. If not, you may have to copy all elements to a new, bigger array.\\
		\hline
		\hline
		Delete (beginning) & $O(n)$ & Have to shift all elements one to the left \\
		Delete (middle) & $O(n)$ & Same as above in the worst case \\
		Delete (end) & $O(1)$ & Free the memory at the end of the array \\
		\hline
	\end{tabularx}
	\begin{tabularx}{\textwidth}{|Y|Y|Y|}
	\vtabularspace{3}
	\hline
	\multicolumn{3}{|c|}{\textbf{Worst-Case Summary}} \\
	\hline
	\textbf{Access = $O(1)$} & \textbf{Search = $O(n)$} & \textbf{Insert/Delete = $O(n)$} \\
	\hline
	\vtabularspace{3}
	\end{tabularx}
\end{table}

A static array has the same search properties, but it cannot insert or delete items, because those operations involve changing the size of the array.

\subsubsection{Linked Lists}

% An interesting example - The Undo & Redo text editing process is a linked list.

A \textit{linked list} is a collection of \textit{nodes} that are not stored contiguously in memory. A node contains some data and a pointer to the next node in the sequence. To \textit{point} a node A to another node B means to assign A's pointer the value of B's location in memory. The first node is called the \textit{head} of the linked list and the last node is called the \textit{tail}. The tail has no next node, so its pointer points to null.\\

The data contained in the nodes of a linked list do not need to have the same type or size. Because the nodes are stored non-contiguously and are accessed via pointers, there is no need for nodes to be of the same size. \\

Linked lists allow for insertion and deletion of elements without restructuring the entire data structure. This means that linked lists are dynamic. To insert an element X between elements P and N, traverse the list to find P, save P's pointer to N as temp, point P to X, and point X to temp. Deleting is a similar process: traverse the list to find P and point it to N. \\

Because you have to traverse the list to insert or delete, it is common practice to keep track of where the tail is located in memory. This allows for constant time insertion of elements at the end of the list. For constant time deletion of the tail, you would also have to cache the penultimate node, Pen, in order to point it to null and make it the new tail. However, keeping track of Pen is $O(n)$ for a singly linked list because there is no way of directly finding the new Pen after deleting the tail once.\\

A linked list can be singly or doubly linked. A doubly linked list has nodes with pointers to both the previous and next nodes in the sequence. This allows for traversal of the list in both directions. It also allows for constant time deletion of the tail, as long as the tail is cached.

\begin{table}[H]
	\caption{Linked List Time Complexity}
	\label{tab:linkedlist}
	\begin{tabularx}{\textwidth}{|c|c|X|}
		\vtabularspace{3}
		\hline
		Operation & Time & \multicolumn{1}{c|}{Reasoning} \\
		\hline
		Access & $O(n)$ & Linked lists do not really have indicies, but you can iterate through a certain amount of pointers, starting at the head \\
		Search & $O(n)$ & Have to check elements linearly \\
		\hline
		\hline
		Insert (beginning) & $O(1)$ & Point the new node to the head \\
		Insert (middle) & $O(i)$ & Search for the previous node at index i-1, then reassign pointers \\
		Insert (end) & \multicolumn{1}{p{3cm}|}{$O(1)$ with caching, $O(n)$ otherwise} & If you know where the tail is stored, you can just point it to the new node. If not, you must first traverse the entire list to find the tail, which is an $O(n)$ operation.\\
		\hline
		\hline
		Delete (beginning) & $O(1)$ & Point head to null \\
		Delete (middle) & $O(i)$ & Search for node at index i, then reassign pointers \\
		Delete (end) & \multicolumn{1}{p{3cm}|}{$O(1)$ with caching and double links, $O(n)$ otherwise} & To delete the tail, you need to know where the penultimate node, Pen, is. If you cache the tail and have double links, you can always find Pen. Otherwise, you must traverse the list to Pen, which is $O(n)$. \\
		\hline
	\end{tabularx}
	\begin{tabularx}{\textwidth}{|Y|Y|Y|}
		\vtabularspace{3}
		\hline
		\multicolumn{3}{|c|}{\textbf{Worst-Case Summary (singly linked, no caching)}} \\
		\hline
		\textbf{Access = $O(n)$} & \textbf{Search = $O(n)$} & \textbf{Insert/Delete = $O(n)$} \\
		\hline
	\end{tabularx}
	\begin{tabularx}{\textwidth}{|Y|Y|Y|}
		\vtabularspace{3}
		\hline
		\multicolumn{3}{|c|}{\textbf{Worst-Case Summary (doubly linked, cached tail)}} \\
		\hline
		\textbf{Access = $O(n)$} & \textbf{Search = $O(n)$} & \textbf{Insert/Delete = $O(1)$} \\
		\hline
		\vtabularspace{3}
	\end{tabularx}
\end{table}

\subsubsection{Skip Lists}

Blah \\

%----------------------------------------

\subsection{Stacks}

A \textit{stack} is a collection of elements with two operations: \textit{push} and \textit{pop}. To push an element onto a stack is to add it to the stack. To pop an element from the stack is to remove its most recently added element and return it. \textit{Peek} is an operation that is sometimes implemented for convenience. It allows access to the most recently added element without removing it from the stack. The most recently added element is located at the \textit{top} of the stack, and the least recently added element is located at the \textit{bottom} of the stack. \\

A stack is similar to a linked list, but its operations are stricter. It can only insert and delete elements at the head of the list (top of the stack), and searching for or accessing an element located somewhere other than the head requires removing elements from head to tail until the desired element is the head. A stack can be implemented using an array or a linked list, so whether or not it is contiguous depends on the implementation details. \\\\

\begin{tcolorbox}[enhanced, colback=textbook-blue, sharp corners]
	\vspace{2mm}
	\begin{center}
		\textbf{Stacks Implemented at the Hardware Level}
	\end{center}
	\vspace{1mm}
	Stacks are also used in the architecture of computer memory. Classically, the bottom of the stack will be placed at a high address in memory and a stack pointer will be assigned its location. When data is pushed onto this stack, the stack will grow downward to lower addresses in memory, and the stack pointer will keep track of the top of the stack (lower addresses). When data is popped, the stack pointer will move accordingly toward the bottom (higher addresses). \\
	
	Local function variables are stored on the stack. If a call to another function is made, that function's local variables will be stored on the stack in a \textit{frame}. From top to bottom (low to high address), the frame of a callee function typically stores local variables, the return address to the code being executed by the caller function, and then parameters of the callee function. The \textit{frame pointer} marks the location of the return address in the frame. This implements the concept of \textit{scope} in computing. \\
\end{tcolorbox}
\vspace{7mm}

\begin{table}[H]
	\caption{Stack Time Complexity}
	\label{tab:stack}
	\begin{tabularx}{\textwidth}{|c|c|X|}
		\vtabularspace{3}
		\hline
		Operation & Time & \multicolumn{1}{c|}{Reasoning} \\
		\hline
		Access & $O(n)$ & To access the bottom element, you must pop every other element \\
		Search & $O(n)$ & If the element you are searching for is at the bottom, you must pop every other element  \\
		\hline
		\hline
		Push & $O(1)$ & Make the new node the head of the list \\
		Pop & $O(1)$ & Remove and return the head and make the next element the new head \\
		Peek & $O(1)$ & Return the head \\
		\hline
		\vtabularspace{3}
	\end{tabularx}
\end{table}

%----------------------------------------

\subsection{Queues}

A queue is a collection of elements with  two operations: \textit{enqueue} and \textit{dequeue}. To enqueue an element is to add it to the queue. To dequeue an element is to remove its least recently added element and return it. As with the stack, \textit{peek} is often implemented for convenience, and it returns the least recently added element. The least recently added element is located at the \textit{front} of the queue, and the most recently added element is located at the \textit{back} of the queue. \\

While a queue can be implemented using an array, it is more commonly implemented using either a singly or doubly linked list or using a dynamic array variant called an "array deque." If implemented using a singly linked list, it must insert elements at the tail and remove them at the head. A queue may or may not be contiguous, depending on the implementation.

\begin{table}[H]
	\caption{Queue Time Complexity}
	\label{tab:queue}
	\begin{tabularx}{\textwidth}{|c|c|X|}
		\vtabularspace{3}
		\hline
		Operation & Time & \multicolumn{1}{c|}{Reasoning} \\
		\hline
		Access & $O(n)$ & To access the back element, you must dequeue every other element \\
		Search & $O(n)$ & If the element you are searching for is at the back, you must dequeue every other element  \\
		\hline
		\hline
		Enqueue & $O(1)$ & Add the new node to the tail of the list \\
		Dequeue & $O(1)$ & Remove and return the head and make the next element the new head \\
		Peek & $O(1)$ & Return the head \\
		\hline
		\vtabularspace{3}
	\end{tabularx}
\end{table}

%----------------------------------------

\subsection{Deques}

Blah

%----------------------------------------

\subsection{Priority Queues}

Blah

%----------------------------------------

\subsection{Graphs}

% Flow networks

A graph is a set of vertices (nodes) and edges between those vertices. Graphs are either undirected or directed, which means their edges are unidirectional or bidirectional, respectively. It is usually implemented using either an \textit{adjacency list} or an \textit{adjacency matrix}. \\

With an adjacency list, the vertices are stored as objects, and each vertex stores a list of its adjacent vertices. The edges could also be stored as objects, in which case, each vertex would store a list of its incident edges, and each edge would store a list of its incident vertices. This implementation could, for example, sufficiently represent a \textit{hypergraph}, which is a generalization of a graph in which an edge can join any number of vertices. \\

With an adjacency matrix, the rows would represent the source vertices and the columns would represent the destination vertices. The matrix simply stores how many edges are incident to both the source vertex and destination vertex. The data pertaining to the vertices and edges is stored outside of the matrix. The adjacency matrix for an undirected graph must be symmetrical, whereas this is not the case for directed graphs. \\

Adjacency lists are better at representing \textit{sparse} graphs (graphs with few edges) while adjacency matrices are better at representing \textit{dense} graphs (graphs with close to the maximum number of edges).

\begin{table}[H]
	\caption{Adjacency List Time Complexity}
	\label{tab:adjlist}
	\begin{tabularx}{\textwidth}{|c|c|X|}
		\vtabularspace{3}
		\hline
		Operation & Time & \multicolumn{1}{c|}{Reasoning} \\
		\hline
		Insert vertex & $O(1)$ & Store vertex in hash table, map it to adjacent vertices \\
		Insert edge & $O(1)$ & Add an adjacent vertex \\
		Remove vertex & $O(|V|)$ & Visit all adjacent vertices of the given vertex, remove the given vertex from their adjacency lists, remove given vertex from map \\
		Remove edge & $O(1)$ & Remove destination vertex from source vertex's list of adjacent vertices \\
		Check adjacency & $O(|V|)$ & In the worst case, a vertex could have an adjacency list containing all vertices in the graph \\
		\hline
		\hline
		\multicolumn{3}{c}{Space complexity: $O(|V|+|E|)$} \\
		\hline
		\hline
	\end{tabularx}
\end{table}

\begin{table}[H]
	\caption{Adjacency Matrix Time Complexity}
	\label{tab:adjmatrix}
	\begin{tabularx}{\textwidth}{|c|c|X|}
		\vtabularspace{3}
		\hline
		Operation & Time & \multicolumn{1}{c|}{Reasoning} \\
		\hline
		Insert vertex & $O(|V^2|)$ & Matrix must be resized \\
		Insert edge & $O(1)$ & Increment value in matrix \\
		Remove vertex & $O(|V^2|)$ & Matrix must be resized \\
		Remove edge & $O(1)$ & Decrement value in matrix \\
		Check adjacency & $O(1)$ & Check if value in matrix is greater than zero \\
		\hline
		\hline
		\multicolumn{3}{c}{Space complexity: $O(|V|^2)$} \\
		\hline
		\hline
		\vtabularspace{3}
	\end{tabularx}
\end{table}

***Talk about graph coloring and MSTs and stuff*** \\

%----------------------------------------

\subsection{Trees}

% Talk about a "single branch tree" or a "path." Kind of like a linked list, right? Basically, there are two design philosophies to data structures: contiguous and non-contiguous (or node-based).
% Discuss the array implementation of a binary tree.
% Talk about how file systems are trees
% Talk about how this guide is tree-structured (3->3.1->3.1.1)

A \textit{tree} is a collection of nodes that contain data and pointers to child nodes. Each child can only have one parent node. There are many descriptors that can be applied to trees. Those that describe some of the most useful tree implementations are listed below.

\begin{table}[H]
	\begin{threeparttable}
		\caption{Tree Descriptors}
		\label{tab:tree}
		\begin{tabularx}{\textwidth}{|c|X|}
			\vtabularspace{2}
			\hline
			Descriptor & \multicolumn{1}{c|}{Meaning} \\
			\hline
			Binary & Each node in the tree has at most 2 children \\
			Balanced & The left and right subtrees of each node differ in height by no more than one \\
			Ordered (or sorted) & The nodes are sorted in some way, such that they can be searched in $O(log\;n)$ time \\
			Complete* & Every level of the tree has the maximum amount of nodes possible, except for, perhaps, the last level \\
			Full* & Each node in the tree has either zero or two children \\
			Perfect* & Each non-leaf node has two children, and all leaves have the same depth \\
			\hline
		\end{tabularx}
		\vspace*{1mm}
		\begin{tablenotes}\footnotesize
			\item[*] These terms are not standardized, but they are often defined this way.
		\end{tablenotes}
		\vspace*{5mm}
	\end{threeparttable}
\end{table}

We will now discuss a variety of tree-based abstract data types and data structures that have proven to be very useful in software design. We will cover the binary search tree, the binary heap, and the trie, as well the preferred implementations for each.

\subsubsection{Binary Search Trees}

A \textit{binary search tree} (BST) refers to an ordered binary tree that satisfies the \textit{binary search property}, which states that each parent node must be greater than or equal to the nodes in its left subtree and less than the nodes in its right subtree. This is a sorted order that is imposed on the tree to give it $O(log\;n)$ search, insert, and delete operations, as long as the tree is also balanced. \\

Inserting a node, $N$, into a binary search tree involves searching for it in the tree until you arrive at a leaf and then making $N$ a child of that leaf. Deleting a node, $N$, is more complicated. If $N$ has no children, simply remove it. If it has one child, replace it with that child. If it has two children, copy the data of $C$, which can be either $N$'s in-order predecessor or in-order successor, to $N$. If $C$ is a leaf, remove it. Otherwise, $C$ has one child. Replace $C$ with that child. \\

\begin{table}[H]
	\caption{Binary Search Tree Time Complexity}
	\label{tab:bst}
	\begin{tabularx}{\textwidth}{|c|c|X|}
		\vtabularspace{3}
		\hline
		Operation & Time & \multicolumn{1}{c|}{Reasoning} \\
		\hline
		Search & $O(log\;n)$ & Perform a binary search \\
		Insert & $O(log\;n)$ & Perform a binary search for the closest node and insert the given node as its leaf \\
		Delete & $O(log\;n)$ & Perform a binary search to find the given node, delete it, and rearrange the tree \\
		\hline
		\vtabularspace{3}
	\end{tabularx}
\end{table}

As nodes are inserted into and deleted from a binary search tree, the tree may become unbalanced. For this reason, \textit{self-balancing BSTs}, such as \textit{AVL trees} and \textit{red-black trees}, are often the preferred BST implementations because a BST's search, insert, and delete operations are only $O(log\;n)$ if the tree remains balanced. \\

\paragraph{AVL Trees} \hspace*{1mm} \vspace*{2mm} \\

In an AVL tree, each node, $N$, stores the heights of its left and right subtrees, $L$ and $R$. The difference between these heights ($L$.\texttt{height} - $R$.\texttt{height}) is called $N$'s \textit{balance}. If its balance is less than -1 or greater than 1, $N$ is unbalanced and must be fixed using \textit{tree rotation}. In practice, if a node becomes unbalanced after an insertion, its balance will be either 2 (L is taller) or -2 (R is taller). An AVL tree is only balanced when all of its nodes are balanced. \\

There are two types of rotations, left and right, and they are inverse operations of each other. A rotation is a way to move a parent ($A$) down the tree while moving its child ($B$) up and preserving the order of the tree. Regardless of the direction, this results in $B$ abandoning its child $C$ to adopt $A$ as its child and $A$ adopting $C$ as its child. During a right rotation, $B$ starts as $A$'s left child, $C$ starts as $B$'s right child, $A$ becomes $B$'s right child, and $C$ becomes $A$'s left child. During a left rotation, $B$ starts as $A$'s right child, $C$ starts as $B$'s left child, $A$ becomes $B$'s left child, and $C$ becomes $A$'s right child. The rotation operation takes the parent node, $A$, as input.\\

If node $N$ is unbalanced, rotations must be performed to balance its subtrees. The pseudocode for this algorithm is given below. \\

\begin{algorithm}[H]
	\KwData{A tree rooted at $N$}
	\Begin{
		\If{N is not balanced}{
			$L \longleftarrow N.left$; \\
			$R \longleftarrow N.right$; \\
			\If{$L$ is taller than $R$}{
				\If{$L$'s extra node is on the right}{
					leftRotation($L$);
				}
				rightRotation($N$);
			}
			\ElseIf{$R$ is taller than $L$}{
				\If{$R$'s extra node is on the left}{
					rightRotation($R$);
				}
				leftRotation($N$);
			}
		}
	}
	\caption{Balancing a node in an AVL Tree}
\end{algorithm}
\vspace{5mm}

When a node is added to the AVL tree with a BST insert operation, this algorithm is applied to all of its ancestors as the recursive call stack moves up the tree. This ensures that the tree is balanced. Deletion is handled similarly. A BST delete operation is performed on a node, and then the balancing algorithm is applied to all of its ancestors recursively. \\

\paragraph{Red-Black Trees} \hspace*{1mm} \vspace*{2mm} \\

A red-black tree is another kind of self-balancing binary search tree in which each node stores an extra bit that colors the node red or black. A red-black tree maintains its balance such that search, insert, and delete operations remain $O(log\;n)$, but its balances requirements are looser. Whereas an AVL tree can only have a max difference in height of 1 between subtrees, a red-black tree can have subtrees whose heights differ by up to a factor of two. \\

In addition to the standard properties of a binary search tree, a red-black tree has five properties that enforce its balance:

\begin{enumerate}
	\item Each node is either red or black.
	\item The root is black.
	\item All leaves, which are null nodes, are black.
	\item Every red node must have two black children.
	\item Every path from a given node to a descendant leaf must have the same number of black nodes.
\end{enumerate}

Consider two paths from a node to its leaves, one with the minimum possible number of red nodes and one with the maximum possible number of red nodes. Each path must have the same number of black nodes, $b$. The minimum number of red nodes is zero, so the first path has $b$ nodes. Because red nodes must have black children and because each path must have $b$ black nodes, the second path has a maximum number of $b$ red parents and $2b$ nodes total. Thus, two paths from a given node to its leaves differ in height by at most a factor of 2, which preserves the efficiency of the BST's operations. \\

Inserting and deleting nodes can be complicated with red-black trees because all five red-black properties must be preserved. The operations required to preserve them depend on the colors of nodes related to the node being inserted or deleted.

\subparagraph{Red-Black Insertion} \hspace*{1mm} \vspace*{2mm} \\
A new node, $N$, replaces a leaf and is \textit{always} initially colored red. It is also given two black, null leaves as children. Because we are inserting a red node, Property 5 is not at risk of being violated, but Property 4 is. We must rebalance the tree in cases where the insertion of $N$ causes a red node to have a red child. In these cases, $N$'s parent, $P$, must be red. We can also assume that $N$'s grandparent, $G$, is black, because its child, $P$, is red. However, $G$'s other child ($N$'s uncle), $U$, could be red or black. \\

\begin{description}[1cm]
	\item[\underline{Case 1}:] $N$ is the root of the tree. \\
	
	Just change $N$'s color to black to comply with Property 2. \\
	
	\item[\underline{Case 2}:] $P$ is black. \\
	
	Property 4 cannot be violated, so no rebalancing is required. \\
	
	\item[\underline{Case 3}:] $P$ is red and $U$ is red. \\
	
	Property 4 is violated because $P$ is red and its child, $N$, is also red. This can be fixed by flipping $G$ to red and flipping $P$ and $U$ to black. However, $G$ may now violate Property 2 (if it is the root) or Property 4 (if its parent is red). For this reason, the rebalancing function (which was called on $N$) should be called recursively on $G$ whenever a Case 3 situation occurs. \\
	
	\item[\underline{Case 4}:] $P$ is red and $U$ is black. \\
	
	Property 4 is violated because $P$ is red and its child, $N$, is also red. In this case, it is significant whether or not $N$ and $P$ are left or right children of their respective parents. This leads to four subcases that are all handled with tree rotations, similar to AVL insertion. \\
	
	\begin{description}[1cm]
		\item[\underline{Case 4.A}:] $N$ and $P$ are left children. \\
		
		Perform a right rotation on $G$ and flip the colors of $P$ and $G$. \\
		
		\item[\underline{Case 4.B}:] $N$ is a right child, and $P$ is a left child. \\
		
		Perform a left rotation on $P$ to create a Case 4.A subtree rooted at $G$. Perform the Case 4.A steps. \\
		
		\item[\underline{Case 4.C}:] $N$ and $P$ are right children. \\
		
		Perform a left rotation on $G$ and flip the colors of $P$ and $G$. \\
		
		\item[\underline{Case 4.D}:] $N$ is a left child, and $P$ is a right child. \\
		
		Perform a right rotation on $P$ to create a Case 4.C subtree rooted at $G$. Perform the Case 4.C steps. \\
	\end{description}
\end{description}

The logic of red-black balancing after insertion can be summarized by the following algorithms: \\\\

\begin{algorithm}[H]
	\SetKwFunction{RBInsertBalance}{void rbInsertBalance}
	\SetKwProg{Fn}{}{:}{}
	\Fn{\RBInsertBalance{$N$}}{
		\KwData{A red node, $N$, that was just inserted into the red-black tree}
		\Begin{
			$P \longleftarrow $ \texttt{parent($N$)}; \\
			$G \longleftarrow $ \texttt{grandparent($N$)}; \\
			$U \longleftarrow $ \texttt{uncle($N$)}; \\
			\If{$P$ == \textsc{null}}{
				\texttt{case1($N$)}; \\
			}
			\ElseIf{$P$ is black}{
				\texttt{case2($N$)};
			}
			\ElseIf{$P$ is red and $U$ is red}{
				\texttt{case3($N$,$P$,$U$,$G$)};
			}
			\Else(\quad\texttt{\textbackslash\textbackslash\ $P$ is red and $U$ is black}){
				\texttt{case4($N$,$P$,$G$)};
			}
		}
	}
	\caption{Red-Black Balancing After Insertion}
\end{algorithm}
\vspace{5mm}

\begin{algorithm}[H]
	\SetKwFunction{CaseOne}{void case1}
	\SetKwFunction{CaseTwo}{void case2}
	\SetKwFunction{CaseThree}{void case3}
	\SetKwFunction{CaseFour}{void case4}
	\SetKwFunction{CaseFourA}{void case4A}
	\SetKwFunction{CaseFourB}{void case4B}
	\SetKwFunction{CaseFourC}{void case4C}
	\SetKwFunction{CaseFourD}{void case4D}
	\SetKwProg{Fn}{}{:}{}
	\Fn{\CaseOne{$N$}}{
		\Begin{
			Color $N$ black;
		}
	}
	\Fn{\CaseTwo{$N$}}{
		\Begin{
			\Return; \quad\tcp{tree is already balanced}
		}
	}
	\Fn{\CaseThree{$N$,$P$,$U$,$G$}}{
		\Begin{
			Color $P$ black; \\
			Color $U$ black; \\
			Color $G$ red; \\
			\texttt{rbInsertBalance($G$)};
		}
	}
	\Fn{\CaseFour{$N$,$P$,$G$}}{
		\Begin{
			\If{$P$ is a left child}{
				\If{$N$ is a right child}{
					\texttt{leftRotation($P$)};
				}
				\texttt{case4A($P$,$G$)};
			}
			\Else{
				\If{$N$ is a left child}{
					\texttt{rightRotation($P$)}; \\
				}
				\texttt{case4C($P$,$G$)};
			}
		}
	}
	\Fn{\CaseFourA{$P$,$G$}}{
		\Begin{
			\texttt{rightRotation(G)}; \\
			Color $P$ black; \\
			Color $G$ red;
		}
	}
	\Fn{\CaseFourC{$P$,$G$}}{
		\Begin{
			\texttt{leftRotation(G)}; \\
			Color $P$ black; \\
			Color $G$ red;
		}
	}
	\caption{Red-Black Balancing Cases}
\end{algorithm}
\vspace{5mm}

The process for balancing after deleting is similar, but it is more complicated and involves more cases. As such, it is not worth the space required to expound on it here. Refer instead to the description on Wikipedia's  \href{https://en.wikipedia.org/wiki/Red-black_tree#Removal}{red-black tree page}.

\subsubsection{Binary Heaps}

A \textit{binary heap} is a complete ordered binary tree that satisfies the \textit{heap property}, which states that each parent node is either greater than or equal to its children (in the case of a \textit{max-heap}) or less than or equal to its children (in the case of a \textit{min-heap}). \\

Binary heaps are often implemented using static or dynamic arrays. The root of the heap is stored at index $0$. Each of the other nodes is stored at an index $i > 0$ such that the locations of its parent, left child, and right child can be calculated according to the following expressions.

\begin{table}[H]
	\centering
	\caption{Relative Indices for a Heap Represented by An Array}
	\label{tab:heap-indicies}
	\begin{tabular}{c|c}
		\vtabularspace{2}
		Node & Index \\
		\hline
		Current Node & $i$ \\
		Parent & $(i-1)/2$ \\
		Left Child & $(2\times i)+1$ \\
		Right Child & $(2\times i)+2$ \\
		\vtabularspace{2}
	\end{tabular}
\end{table}

Assuming an array implementation, we can insert a new node in the heap by adding it to the end of the array and resolving any violations of the heap property that occur, if any. We can also delete any node in the heap (including the root) by removing it, replacing it with the last node in the array, and resolving any violations of the heap property that occur, if any. A heap is an optimal choice for implementing a priority queue, which has fundamental operations such as insert, find min/max, and delete min/max.

\begin{table}[H]
	\caption{Binary Heap Time Complexity}
	\label{tab:heap}
	\begin{tabularx}{\textwidth}{|c|c|X|}
		\vtabularspace{3}
		\hline
		Operation & Time & \multicolumn{1}{c|}{Reasoning} \\
		\hline
		Insert & $O(log\;n)$ & Insert node at end of array and "bubble up" \\
		Find Min/Max & $O(1)$ & The min/max (depends on heap order) is always stored at index 0 \\
		Delete Min/Max & $O(log\;n)$ & Replace root with last node and "bubble down" \\
		\hline
		\vtabularspace{3}
	\end{tabularx}
\end{table}

\subsubsection{Tries}

A \textit{trie} is an ordered tree that is typically not binary. It is similar to a map, but the keys (which are usually strings) are not necessarily associated with every node. Instead, a node may store a prefix of a key (such as the first character of a key) and have an incident edge that points to a key or to another prefix that adds a single character to the original prefix. The root node stores an empty string.\\

This allows for the compact storage and convenient search of strings that share prefixes. This would be useful for storing a dictionary of English words and recommending valid words given a prefix. For this reason, it is often used for autocompleting words and storing new, custom words. \\

\begin{table}[H]
	\caption{Trie Time Complexity}
	\label{tab:trie}
	\begin{tabularx}{\textwidth}{|c|c|X|}
		\vtabularspace{3}
		\hline
		Operation & Time & \multicolumn{1}{c|}{Reasoning} \\
		\hline
		Search & $O(n)$ & Traverse the trie character-by-character until the full $n$-length string is found or you hit a leaf \\
		Insert & $O(n)$ & Add new prefix nodes to the trie character-by-character until there is an $n$-length path leading from the root to the given word \\
		Delete & $O(n)$ & Traverse the trie toward the given string and delete the first node you encounter with an out-degree of 1.\\
		\hline
		\vtabularspace{3}
	\end{tabularx}
\end{table}

%----------------------------------------

\subsection{Maps}

A \textit{map}, also known as an \textit{associative array}, is a collection of key-value pairs, such that keys are unique. A key can be used to search the map to find its corresponding value. Maps are typically implemented with a hash table (to make a hash map) or a tree (to make a tree map). Whether or not a map has order depending on its implementation. Hash maps are not ordered while tree maps are ordered.

\subsubsection{Hash Maps}

A hash table uses a hash function to map keys to \textit{buckets} in an array. It does this by hashing a key and reducing that hash to an index in the array, using a modulo operator (\texttt{index = hash \% array\_size}). It must also handle \textit{collisions}, which occur when two or more keys map to the same bucket. The two common ways of handling this are \textit{separate chaining} and \textit{open addressing}.

\paragraph{Separate Chaining} \hspace*{1mm} \\
In separate chaining, buckets hold pointers to linked lists, which hold the key-value pairs in nodes. A bucket without a collision will point to a single node, whereas a bucket with a collision will point to a chain of nodes. The buckets could also instead hold the heads of the linked lists, which would decrease the number of pointer traversals by 1, but would also increase the size of the buckets, including the empty ones, if the values take up more space than a pointer.

\paragraph{Open Addressing} \hspace*{1mm} \\
In open addressing, all key-value pairs are stored in the bucket array. When a new pair has to be inserted, the bucket corresponding to its hashed key (\textit{preferred bucket}) is checked for occupancy. If there is a collision, more buckets are checked in a \textit{probe sequence}. The most common probe sequence is \textit{linear probing}, which means you check buckets separated by a fixed interval, which is usually 1 (bucket x, bucket x+1, bucket x+2, ...). When an unoccupied bucket is found, the pair is stored there. When a key is used to access a value, the key is compared to the key stored in the preferred bucket. If it matches, the value in that bucket is returned. Otherwise, the process is repeated with other buckets according to the probe sequence and will return either when the keys match (item found) or when the bucket is empty (item not found).

\paragraph{Dynamic Resizing} \hspace*{1mm} \\
A hash table that is quite full is slower than one that is quite empty, regardless of the style of collision resolution. If table that uses separate chaining is quite full, it is more likely that future items will have to be chained to existing items, which leads to longer search times. If a table that uses open addressing is quite full, a future item whose preferred bucket is filled will spend longer looking for an empty bucket, and it will take longer to search for it as well. \\

The load factor of a hash table is the ratio of the number of keys stored in the table to the number of buckets in the table. When the load factor exceeds some limit (0.75 is commonly used), the hash table will be \textit{rehashed}, which involves creating a new hash table and remapping all of the elements to it. There are other techniques that allow for incremental resizing instead of this all-at-once method, which can be useful for highly available systems.

\begin{table}[H] 
	\begin{threeparttable}
		\caption{Hash Map Time Complexity}
		\label{tab:hashmap}
		\begin{tabularx}{\textwidth}{|c|c|X|}
			\vtabularspace{3}
			\hline
			Operation & Time & \multicolumn{1}{c|}{Reasoning} \\
			\hline
			Access & N/A & Hash maps have no total order \\
			Search & $O(1)$* & The hash tells you where the data is located \\
			Insert & $O(1)$* & The hash tells you where to put the data \\
			Delete & $O(1)$* & The hash tells you which bucket to empty \\
			\hline
		\end{tabularx}
		\vspace*{1mm}
		\begin{tablenotes}\footnotesize
			\item[*] This time complexity assumes a good hash function with minimal collisions. A very bad hash function with result in an $O(n)$ time complexity.
		\end{tablenotes}
		\vspace*{5mm}
	\end{threeparttable}
\end{table}

\subsubsection{Tree Maps}

A perfect hash function results in an $O(1)$ search time. A particularly bad hash function could cause a hash table with separate chaining to put all key-value pairs into the same bucket, which would result in an $O(n)$ search time. A tree implementation lands in between these scenario with an $O(log\;n)$ search time. It can be visualized as a hash table with one bucket which contains the root of a tree instead of the head of a linked list. \\

Maps like this are often implemented using self-balancing binary search trees like AVL trees or red-black trees. While they are less time-efficient than maps that have good hash functions, they are sorted by key, and thus allow for fast enumeration of items in key order. However, their search algorithms become complicated with the presence of collisions.

\begin{table}[H]
	\caption{Tree Map Time Complexity}
	\label{tab:treemap}
	\begin{tabularx}{\textwidth}{|c|c|X|}
		\vtabularspace{3}
		\hline
		Operation & Time & \multicolumn{1}{c|}{Reasoning} \\
		\hline
		Access & N/A & Tree maps have no single \textit{unambiguous} total order \\
		Search & $O(log\;n)$ & Traverse the search tree \\
		Insert & $O(log\;n)$ & Traverse the search tree and insert the value\\
		Delete & $O(log\;n)$ & Traverse the search tree and delete the value, reorganizing the tree if necessary \\
		\hline
		\vtabularspace{3}
	\end{tabularx}
\end{table}

%----------------------------------------

\subsection{Sets}

A set is an unordered collection of unique elements. It is typically implemented in a similar way to a map. A hash table is used for unsorted sets to achieve $O(1)$ search/insert/delete. A self-balancing binary search tree is used for sorted sets to achieve $O(log n)$ search/insert/delete and fast enumeration in sorted order. \\

A set can also be implemented using a map. In this case, the keys are the elements and the values are flags, such as $1$ or \texttt{true}. \\

%----------------------------------------

\subsection{Multisets (or Bags)}

Blah
